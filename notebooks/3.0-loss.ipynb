{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n",
    "\n",
    "\\begin{equation} L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2 \\end{equation}\n",
    "\n",
    "$\\hat{y}_{u}$: predicted score of a positive sample\n",
    "\n",
    "$\\hat{y}_{uj}$: predicted score of a negative sample\n",
    "\n",
    "$\\lambda$: hyperparameter which controls the L2 regularization strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(\n",
    "    users_emb_final,\n",
    "    users_emb_0,\n",
    "    pos_items_emb_final,\n",
    "    pos_items_emb_0,\n",
    "    neg_items_emb_final,\n",
    "    neg_items_emb_0,\n",
    "    lambda_val,\n",
    "):\n",
    "    reg_loss = lambda_val * (\n",
    "        users_emb_0.norm(2).pow(2)\n",
    "        + pos_items_emb_0.norm(2).pow(2)\n",
    "        + neg_items_emb_0.norm(2).pow(2)\n",
    "    )  # L2 loss\n",
    "\n",
    "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
    "    pos_scores = torch.sum(pos_scores, dim=-1)  # predicted scores of positive samples\n",
    "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
    "    neg_scores = torch.sum(neg_scores, dim=-1)  # predicted scores of negative samples\n",
    "\n",
    "    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
