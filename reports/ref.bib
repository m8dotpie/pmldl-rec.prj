@misc{Grouplens,
  title = {Grouplens}
}

@misc{heLightGCNSimplifyingPowering2020,
  title = {{{LightGCN}}: {{Simplifying}} and {{Powering Graph Convolution Network}} for {{Recommendation}}},
  shorttitle = {{{LightGCN}}},
  author = {He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, Yongdong and Wang, Meng},
  year = {2020},
  month = jul,
  number = {arXiv:2002.02126},
  eprint = {2002.02126},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-03},
  abstract = {Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance. In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0{\textbackslash}\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/XJP328HY/He et al. - 2020 - LightGCN Simplifying and Powering Graph Convoluti.pdf;/Users/m8dotpie/Zotero/storage/UC82UCI2/2002.html}
}

@misc{LightGCNPytorch,
  title = {{{LightGCN-Pytorch}}}
}

@misc{PyG,
  title = {{{PyG}}}
}

@misc{PyTorch,
  title = {{{PyTorch}}}
}

@misc{rendleBPRBayesianPersonalized2012,
  title = {{{BPR}}: {{Bayesian Personalized Ranking}} from {{Implicit Feedback}}},
  shorttitle = {{{BPR}}},
  author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and {Schmidt-Thieme}, Lars},
  year = {2012},
  month = may,
  number = {arXiv:1205.2618},
  eprint = {1205.2618},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-03},
  abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/GKNWNG9S/Rendle et al. - 2012 - BPR Bayesian Personalized Ranking from Implicit F.pdf;/Users/m8dotpie/Zotero/storage/GV8WDHYK/1205.html}
}

@misc{xuHowPowerfulAre2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  month = feb,
  number = {arXiv:1810.00826},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-03},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/XH3NLPSS/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/Users/m8dotpie/Zotero/storage/3LPZ9N72/1810.html}
}

@article{zhouGraphNeuralNetworks2020,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year = {2020},
  journal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2021.01.001},
  urldate = {2023-12-03},
  langid = {english},
  file = {/Users/m8dotpie/Zotero/storage/XFLAQMAQ/Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf}
}
