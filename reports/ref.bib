@misc{Grouplens,
  title = {Grouplens}
}

@misc{heLightGCNSimplifyingPowering2020,
  title = {{{LightGCN}}: {{Simplifying}} and {{Powering Graph Convolution Network}} for {{Recommendation}}},
  shorttitle = {{{LightGCN}}},
  author = {He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, Yongdong and Wang, Meng},
  year = {2020},
  month = jul,
  number = {arXiv:2002.02126},
  eprint = {2002.02126},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance. In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0{\textbackslash}\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/XJP328HY/He et al. - 2020 - LightGCN Simplifying and Powering Graph Convoluti.pdf;/Users/m8dotpie/Zotero/storage/UC82UCI2/2002.html}
}

@misc{jeunenNormalisedDiscountedCumulative2023,
  title = {On ({{Normalised}}) {{Discounted Cumulative Gain}} as an {{Off-Policy Evaluation Metric}} for {{Top-}}\$n\$ {{Recommendation}}},
  author = {Jeunen, Olivier and Potapov, Ivan and Ustimenko, Aleksei},
  year = {2023},
  month = nov,
  number = {arXiv:2307.15053},
  eprint = {2307.15053},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-\$n\$ recommendation for many years. Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/UBWHEBPJ/Jeunen et al. - 2023 - On (Normalised) Discounted Cumulative Gain as an O.pdf;/Users/m8dotpie/Zotero/storage/4MCWHY3U/2307.html}
}

@misc{LightGCNPytorch,
  title = {{{LightGCN-Pytorch}}}
}

@misc{PyG,
  title = {{{PyG}}}
}

@misc{PyTorch,
  title = {{{PyTorch}}}
}

@misc{rendleBPRBayesianPersonalized2012,
  title = {{{BPR}}: {{Bayesian Personalized Ranking}} from {{Implicit Feedback}}},
  shorttitle = {{{BPR}}},
  author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and {Schmidt-Thieme}, Lars},
  year = {2012},
  month = may,
  number = {arXiv:1205.2618},
  eprint = {1205.2618},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/GKNWNG9S/Rendle et al. - 2012 - BPR Bayesian Personalized Ranking from Implicit F.pdf;/Users/m8dotpie/Zotero/storage/GV8WDHYK/1205.html}
}

@misc{xuHowPowerfulAre2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  month = feb,
  number = {arXiv:1810.00826},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/XH3NLPSS/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/Users/m8dotpie/Zotero/storage/3LPZ9N72/1810.html}
}

@misc{yangUnderstandingNegativeSampling2020,
  title = {Understanding {{Negative Sampling}} in {{Graph Representation Learning}}},
  author = {Yang, Zhen and Ding, Ming and Zhou, Chang and Yang, Hongxia and Zhou, Jingren and Tang, Jie},
  year = {2020},
  month = jun,
  number = {arXiv:2005.09863},
  eprint = {2005.09863},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Graph representation learning has been extensively studied in recent years. Despite its potential in generating continuous embeddings for various networks, both the effectiveness and efficiency to infer high-quality representations toward large corpus of nodes are still challenging. Sampling is a critical point to achieve the performance goals. Prior arts usually focus on sampling positive node pairs, while the strategy for negative sampling is left insufficiently explored. To bridge the gap, we systematically analyze the role of negative sampling from the perspectives of both objective and risk, theoretically demonstrating that negative sampling is as important as positive sampling in determining the optimization objective and the resulted variance. To the best of our knowledge, we are the first to derive the theory and quantify that the negative sampling distribution should be positively but sub-linearly correlated to their positive sampling distribution. With the guidance of the theory, we propose MCNS, approximating the positive distribution with self-contrast approximation and accelerating negative sampling by Metropolis-Hastings. We evaluate our method on 5 datasets that cover extensive downstream graph learning tasks, including link prediction, node classification and personalized recommendation, on a total of 19 experimental settings. These relatively comprehensive experimental results demonstrate its robustness and superiorities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/8PULNUYC/Yang et al. - 2020 - Understanding Negative Sampling in Graph Represent.pdf;/Users/m8dotpie/Zotero/storage/YTIQEQN2/2005.html}
}

@article{zhouGraphNeuralNetworks2020,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year = {2020},
  journal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2021.01.001},
  langid = {english},
  file = {/Users/m8dotpie/Zotero/storage/XFLAQMAQ/Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf}
}
